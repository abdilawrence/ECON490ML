---
title: "ECON 490ML Final Project Report"
author: "Abdi Lawrence, Henry Chen, Matthew Hong"
date: "4/26/2022"
header-includes:
    - \usepackage{setspace}\doublespacing
    - \usepackage{float}
output:
  pdf_document: default
  html_document: default
---

## Introduction

For our Final Project, we will be attempting to predict the yield of wild blueberries. These are known as "lowbush berries", which grow on low-level bushes and are typically pea-sized. Our dataset can be found at **https://www.kaggle.com/datasets/saurabhshahane/wild-blueberry-yield-prediction**. This data was generated by a simulation model called the Wild Blueberry Pollination Model, which has been validated by experimental data collected in the State of Maine over the last 30 years.

Generally, in the field of agriculture, an increasing quantity of research has gone underway to understand the determinants of crop yield. Machine learning has enabled scientists and farmers to investigate which factors have the greatest impact on crop yield. In this specific case, the crop of interest is the wild blueberry. The paper for which this data was created is called "Simulation-based modeling of wild blueberry pollination". It was published in the January 2018 version of *Computers and Electronics in Agriculture*. The paper can be found at: **https://www.sciencedirect.com/science/article/pii/S0168169916310274?via%3Dihub**.

## Handling Data

Below the dataset will be loaded from a .csv file. The `tidyverse` library has been loaded so that the data will automatically be loaded as a tibble, which provides some advantages over a standard data frame. One advantage being that the `read_csv` function will automatically assign types to each column in the tibble.

```{r, eval = TRUE, message=FALSE, warning=FALSE}

library(tidyverse)
data = read_csv("blueberryData.csv")
colnames(data)
dim(data)

```

The dataset contains 18 columns and 777 observations. Along with downloaded the dataset from Kaggle, we downloaded the descriptions of the predictor variables in the dataset.

![](predictors.jpg)

There are 13 predictor variables in the dataset. Their information includes clone size (size of bush), bee density by species, temperature, and precipitation. `fruitset, fruitmass, seeds`, and `yield` are all response variables. All predictor and response variables are quantitative. There are no qualitative variables. `Row#` can be deleted because it serves no purpose.

```{r, eval = TRUE, message=FALSE, warning=FALSE}
data = select(data, -'Row#')
dim(data)
```

The dataset does not have any `NA` values. We also will not need to convert any columns into factors, since there is no categorical data.

```{r, eval = TRUE, message=FALSE, warning=FALSE}
sum(is.na(data))
```

## Exploration

A look at the density plots of 9 of our predictor variables shows that none of the distributions are approximately normal. Although this will not be detrimental to our algorithms, this was a surprise considering this is a simulation of natural factors (temperature, bees, precipitation). 

```{r, eval = TRUE, message=FALSE, warning=FALSE}
library(gridExtra)

plot1 = ggplot(data, aes(clonesize)) + geom_density()
plot2 = ggplot(data, aes(honeybee)) + geom_density() + xlim(0, 1)
plot3 = ggplot(data, aes(bumbles)) + geom_density()
plot4 = ggplot(data, aes(andrena)) + geom_density()
plot5 = ggplot(data, aes(osmia)) + geom_density()
plot6 = ggplot(data, aes(AverageOfUpperTRange)) + geom_density()
plot7 = ggplot(data, aes(AverageOfLowerTRange)) + geom_density()
plot8 = ggplot(data, aes(RainingDays)) + geom_density()
plot9 = ggplot(data, aes(AverageRainingDays)) + geom_density()

grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6, plot7, plot8, plot9, ncol = 3)
```

## Prediction

We will attempt to predict `yield` with the given predictor variables in the dataset. We chose `yield` as opposed to the other response variables because it is measured in quantity of blueberries, which we felt to the most important measure of a harvest.

Before beginning any machine learning algorithms, we decided to only retain `AverageOfLowerTRange` and `AverageOfUpperTRange` among out six temperature variables. The researchers used historical weather data to simulate the ranges of temperatures. Because of this we are comfortable using these two variables as our high and low temperatures. Our decision to remove these variables will simplify the model, and minimize multicolinearity. This will result in 9 predictor variables for our modeling.

```{r, eval = TRUE, message=FALSE, warning=FALSE}
data = select(data, -c(MaxOfUpperTRange, MinOfUpperTRange, MaxOfLowerTRange,fruitset, fruitmass, seeds, MinOfLowerTRange))
dim(data)
```

We will also create training and test sets from our data before starting our modeling. Out split will be 70% training, 30% testing.

```{r, eval = TRUE, message=FALSE, warning=FALSE}
set.seed(42)
train = sample(c(TRUE, FALSE), size = nrow(data), prob = c(0.7, 0.3), replace = TRUE)
traindata = data[train, ]
testdata = data[!train, ]
```

We will compare the performance of our models based on their RMSE values, which is Root Mean Squared Error.

### Multiple Linear Regression

Our first method will be to model the data using Multiple Linear Regression, with all 9 predictor variables. The result of the regression is the formula
$$ yield = 7928.955 \ - \ 98.207clonesize \ +\ 118.492honeybee \ +\ 5980.501bumbles \ +\ 520.207adrena \ +\ 2448.218osmia$$ 
$$+\ 236.815AverageOfUpperTRange \ -\ 369.804AverageOfLowerTRange \ +\ 51.718RainingDays $$
$$-\ 8375.642AverageRainingDays$$

Surprisingly, `AverageOfUpperTRange` and `AverageOfLowerTRange` are not statistically significant in this model. Perhaps temperature does not have much an effect yield. It's too early to know for sure, but more evidence may be presented as we continue. This model produced an RMSE value of $590.3259$. Another insight this model has provided is that bumblebees may have the largest impact on blueberry yield of the four species. Lastly, it was not expected that the coefficient of `clonesize` would be negative. We assumed that larger bushes produce more berries on average.

```{r, eval = TRUE, message=FALSE, warning=FALSE}
set.seed(42)
MLR = lm(yield ~ ., data = traindata)
summary(MLR)

MLRtest = predict(MLR, newdata = testdata)
sqrt(mean((MLRtest - testdata$yield) ^ 2)) #RMSE
  
```

### Best Subset Selection

Using best subset selection, we created a model Multiple Linear Regression model with 8 predictor variables. We chose to use 8 variables because this quantity produced the best score for 3 out the 4 measures. This meant that `AverageOfUpperTRange` was omitted from the model. This method produced the formula

$$ yield = 7908.489 \ - \ 98.294clonesize \ +\ 118.673honeybee \ +\ 5962.802bumbles \ +\ 524.529adrena \ +\ 2430.845osmia$$ 
$$-\ 34.181AverageOfLowerTRange \ +\ 52.857RainingDays \ -\ 8461.237AverageRainingDays$$
This model produced an RMSE of $593.2259$ on the testing data, which is slightly worse in performance than our Multiple Linear Regression Model with all predictor variables.

```{r, eval = TRUE, message=FALSE, warning=FALSE}

library(leaps)
set.seed(42)

regfit.full = regsubsets(yield ~ ., data = traindata, nvmax = ncol(data) - 1)
summary(regfit.full)
reg.summary = summary(regfit.full)

par(mfrow = c(2, 2))
plot(reg.summary$adjr2, xlab = "Variables", ylab = "Adj. R^2", type = "l")
points(which.max(reg.summary$adjr2), reg.summary$adjr2[which.max(reg.summary$adjr2)], col = "blue", pch = 20)

plot(reg.summary$cp, xlab = "Variables", ylab = "CP", type = "l")
points(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], col = "blue", pch = 20)

plot(reg.summary$bic, xlab = "Variables", ylab = "BIC", type = "l")
points(which.min(reg.summary$bic), reg.summary$bic[which.min(reg.summary$bic)], col = "blue", pch = 20)

plot(reg.summary$rss, xlab = "Variables", ylab = "RSS", type = "l")
points(which.min(reg.summary$rss), reg.summary$rss[which.min(reg.summary$rss)], col = "blue", pch = 20)

coefficients(regfit.full, 8)
subsetEQ = lm(yield ~ . - AverageOfUpperTRange, data = traindata) #Run regression with only 8 variables

subsetTest = predict(subsetEQ, newdata = testdata)
sqrt(mean((subsetTest - testdata$yield) ^ 2)) #RMSE
```

### Ridge Regression

Using the Ridge Regression technique, we estimated the `yield` value based on all 9 predictor variables. The cross-validation method returned a lambda value of $75.6745$. With this lambda value, we received the formula

$$ yield = 7656.7 \ - \ 92.8clonesize \ +\ 113.78honeybee \ +\ 5423.51bumbles \ +\ 500.52adrena \ +\ 2244.37osmia$$ 
$$-\ 11.47AverageOfUpperTRange \ -\ 16.975AverageOfLowerTRange \ -\ 21.945RainingDays $$
$$-\ 3012.2AverageRainingDays$$

Unsurprisingly, the coefficients for both our temperature related variables are much smaller with ridge regression. Because of the penalty involved, it was expected that these variables would have their effects diminished, as they were not statistically significant in our first method. This method produced an RMSE of $599.4205$, which is slightly higher than both models we've done so far.


```{r, eval = TRUE, message=FALSE, warning=FALSE}
library(glmnet)
train.test = model.matrix(yield~., data = traindata)
x.test = model.matrix(yield~., data = testdata)

     
# Finding lambda chosen by cross-validation
set.seed(42)
ridge.cv = cv.glmnet(train.test, traindata$yield, alpha = 0)
lambda.ridge = ridge.cv$lambda.min
lambda.ridge
plot(ridge.cv)

#Fitting to ridge regression
ridge.fit = glmnet(train.test, traindata$yield, alpha = 0, lambda = lambda.ridge)
coef(ridge.fit)

#Mean square error
ridge.pred = predict(ridge.fit, newx=x.test, s = lambda.ridge)
sqrt(mean((testdata$yield - ridge.pred)^2))
```

### Lasso Regression

With the Lasso Regression technique, we got the best lambda value of $0.586$. Using this lambda value, we received the estimated formula

$$ yield = 7880.01 \ - \ 98.15clonesize \ +\ 119.04honeybee \ +\ 5930.24bumbles \ +\ 519.9adrena \ +\ 2419.37osmia$$
$$-\ 15.3AverageOfUpperTRange \ -\ 12.37AverageOfLowerTRange \ +\ 44.93RainingDays $$
$$-\ 7899.73AverageRainingDays$$

This estimation has an RMSE of $592.97$. Similar to the Ridge Regression, the coefficients of the temperature variables are quite small.

```{r, eval = TRUE, message=FALSE, warning=FALSE}
# Finding lambda chosen by cross-validation
set.seed(42)
lasso.cv = cv.glmnet(train.test, traindata$yield, alpha = 1)
lambda.lasso = lasso.cv$lambda.min
lambda.lasso
plot(lasso.cv)

#Fitting to lasso regression
lasso.fit = glmnet(train.test, traindata$yield, alpha = 1, lambda = lambda.lasso)
coef(lasso.fit)

#Mean square error
lasso.pred = predict(lasso.fit, newx=x.test, s = lambda.lasso)
sqrt(mean((testdata$yield - lasso.pred)^2))
```

### Regression Tree

Below we ran a single regression tree. The tree initially returned 14 nodes. According to the k-fold cross validation technique, pruning the tree does not improve performance. We left our tree unchanged with 14 nodes. This tree produced an RMSE of $650.23$. This RMSE is significantly lower than the methods that have been attempted so far.

```{r, eval = TRUE, message=FALSE, warning=FALSE}
## creating regression tree with training data
library(tree)
set.seed(42)
tree.train =  tree(yield ~ ., data = traindata)
plot(tree.train)
text(tree.train, pretty = 0, cex = 0.45)

##test RMSE prior to CV + pruning
predict.yield1 = predict(tree.train, newdata = testdata)
sqrt(mean((predict.yield1 - testdata$yield)^2))

## apply K-fold cross validation, K = 10 is standard size to use
## graph reveals that the training RSS is at its minimum at 14 nodes
cv.train = cv.tree(tree.train, K = 10)
plot(cv.train$size, cv.train$dev, xlab = "Nodes", ylab = "CV RSS", type = "b")

## prune tree
tree.prune = prune.tree(tree.train, best = 14)

##conclude pruning does not affect RMSE
predict.yield2 = predict(tree.prune, newdata = testdata)
sqrt(mean((predict.yield2 - testdata$yield)^2))
```

### Bagging

Below we ran a bagging regression with trees. According to the `importance` function, the most important variables are `bumbles`, `osmia`, and `clonesize` in that order. This methods returned the lowest RMSE so far, with a value of $381.66$. As expected, bagging with 500 trees performed better than a single tree.

```{r, eval = TRUE, message=FALSE, warning=FALSE}
##Bagging tries to reduce variance which will in turn reduce RMSE
library(randomForest)
set.seed(42)
tree.bag = randomForest(yield ~ ., data = traindata, mtry = 9, importance = T)
##Significantly smaller RMSE relative to what we have done so far
predict.yield3 = predict(tree.bag, newdata = testdata)
varImpPlot(tree.bag)
sqrt(mean((predict.yield3 - testdata$yield)^2))
```

### Random Forests

Lastly, we modeled our data with the Random Forests technique. For each tree, we set the variables selected to $3$. According to the `importance` function, the most important variables are `bumbles`, `AverageOfUpperTRange`, and `AverageOfLowerTRange`. The significance of the temperature variables is likely overvalued as a result of the Random Forests method. Because each tree only used 3 variables, the temperature variables had a higher relative impact. `bumbles` on the other hand, is the most impactful variable, which is consistent with our findings so far. This model performed the best out of all our methods, with an RMSE of 367.44.

```{r, eval = TRUE, message=FALSE, warning=FALSE}
##Random Forests to see if it'll produce a even smaller RMSE
set.seed(42)
tree.rf = randomForest(yield ~ ., data = traindata, mtry = 3, importance = T)
varImpPlot(tree.rf)
predict.yield4 = predict(tree.rf, newdata = testdata)
#RMSE produced by Random Forests is smallest out of all tree methods used
sqrt(mean((predict.yield4 - testdata$yield)^2))

```

## Conclusion

After running several models, we can conclude that a Random Forest method models our data very well. Unfortunately, a formula cannot be derived from the Random Forest method. Despite this fact, we can still extract valuable information from our models with formulas. Our best performing model was Multiple Linear Regression, with all 9 of our variables. This method resulted in the following formula

$$ yield = 7928.955 \ - \ 98.207clonesize \ +\ 118.492honeybee \ +\ 5980.501bumbles \ +\ 520.207adrena \ +\ 2448.218osmia$$ 
$$+\ 236.815AverageOfUpperTRange \ -\ 369.804AverageOfLowerTRange \ +\ 51.718RainingDays $$
$$-\ 8375.642AverageRainingDays$$

We discovered that bumblebees have a substantial impact on blueberry yield. According to our MLR model, for each additional bumblebee per $m^2$ per second, blueberry yield is expected to increase by approximately $5980.5$. According to our Random Forest model, bumblebee density has the greatest impact on blueberry yield.

On the other hand, honeybees do not have much of an impact on blueberry yield. According to our Random Forest models, they are either the least or second least important variable in affecting blueberry yield. This is also the case in our MLR model, in which `bumblebee` only has a coefficient of $118.492$, which is the smallest of any bee species.

Lastly, we discovered that `clonesize` has a negative effect on blueberry yield. We must keep in mind ,however, that the range of `clonesize` values is [10, 40]. This likely means that the optimal size is around 10 squared meters, and as size increases yield is expected to decrease. We cannot expect a bush if size 0.5 squared meters to produce a higher yield than one of size 10 squared meters. Our inference is that a larger size may lead to an inefficient distribution of resources (soil, nutrients, water). Perhaps, due to natural circumstances, a smaller bush can sustain itself better. 

#### References

https://www.kaggle.com/datasets/saurabhshahane/wild-blueberry-yield-prediction

https://www.sciencedirect.com/science/article/pii/S016816992031156X?via%3Dihub